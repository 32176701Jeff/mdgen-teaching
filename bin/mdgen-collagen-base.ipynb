{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40c22084",
   "metadata": {},
   "source": [
    "### preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f74663",
   "metadata": {},
   "source": [
    "#### dcd to pdb and xtc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a10168d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/mnt/hdd/jeff/dataset/output/mdgen-collagen/wh-wt-nc1atm/raw.pdb',\n",
       " '/mnt/hdd/jeff/dataset/output/mdgen-collagen/wh-wt-nc1atm/raw.xtc')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dcd_to_frame0_pdb_and_xtc(\n",
    "    dcd_path,\n",
    "    top_path,\n",
    "    out_pdb,\n",
    "    out_xtc,\n",
    "    stride= 1,):\n",
    "    \"\"\"\n",
    "    Convert a DCD trajectory to:\n",
    "      (1) frame-0 PDB (out_pdb)\n",
    "      (2) XTC trajectory (out_xtc)\n",
    "\n",
    "    Requirements:\n",
    "      pip install mdtraj\n",
    "\n",
    "    Args:\n",
    "      dcd_path: input .dcd\n",
    "      top_path: topology file path (typically .pdb / .prmtop / .psf / etc.)\n",
    "      out_pdb: output PDB filename for the first frame\n",
    "      out_xtc: output XTC filename for the trajectory\n",
    "      stride: keep every `stride` frames (1 = keep all)\n",
    "\n",
    "    Returns:\n",
    "      (out_pdb, out_xtc)\n",
    "    \"\"\"\n",
    "    import os\n",
    "\n",
    "    if not os.path.isfile(dcd_path):\n",
    "        raise FileNotFoundError(f\"DCD not found: {dcd_path}\")\n",
    "    if not os.path.isfile(top_path):\n",
    "        raise FileNotFoundError(f\"Topology not found: {top_path}\")\n",
    "    if stride < 1:\n",
    "        raise ValueError(\"stride must be >= 1\")\n",
    "\n",
    "    try:\n",
    "        import mdtraj as md\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"mdtraj is required. Install with: pip install mdtraj\") from e\n",
    "\n",
    "    traj = md.load_dcd(dcd_path, top=top_path, stride=stride)\n",
    "    if traj.n_frames == 0:\n",
    "        raise RuntimeError(\"Loaded trajectory has 0 frames. Check your DCD/topology.\")\n",
    "\n",
    "    # Save frame 0 as PDB\n",
    "    traj[0].save_pdb(out_pdb)\n",
    "\n",
    "    # Save full trajectory as XTC\n",
    "    traj.save_xtc(out_xtc)\n",
    "\n",
    "    return out_pdb, out_xtc\n",
    "pdb = '/mnt/hdd/jeff/dataset/output/mdgen-collagen/wh-wt-raw/initial.pdb'\n",
    "dcd = '/mnt/hdd/jeff/dataset/output/mdgen-collagen/wh-wt-raw/nc_1atm_last5ns.dcd'\n",
    "out_pdb = '/mnt/hdd/jeff/dataset/output/mdgen-collagen/wh-wt-nc1atm/raw.pdb'\n",
    "out_xtc = '/mnt/hdd/jeff/dataset/output/mdgen-collagen/wh-wt-nc1atm/raw.xtc'\n",
    "dcd_to_frame0_pdb_and_xtc(dcd,pdb,out_pdb,out_xtc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8231daf2",
   "metadata": {},
   "source": [
    "#### remove water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88907ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/mnt/hdd/jeff/dataset/output/mdgen-collagen/wh-wt-nc1atm/no-water.pdb',\n",
       " '/mnt/hdd/jeff/dataset/output/mdgen-collagen/wh-wt-nc1atm/no-water.xtc')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_water_from_pdb_xtc(\n",
    "    in_pdb,\n",
    "    in_xtc,\n",
    "    out_pdb,\n",
    "    out_xtc,\n",
    "    remove_resnames=(\"CLA\",\"SOD\",\"HOH\",\"WAT\",\"SOL\",\"TIP3\",\"TIP3P\",\"SPC\",\"SPCE\"),\n",
    "    stride=1,\n",
    "):\n",
    "    import os\n",
    "    if not os.path.isfile(in_pdb):\n",
    "        raise FileNotFoundError(f\"PDB not found: {in_pdb}\")\n",
    "    if not os.path.isfile(in_xtc):\n",
    "        raise FileNotFoundError(f\"XTC not found: {in_xtc}\")\n",
    "\n",
    "    import mdtraj as md\n",
    "\n",
    "    traj = md.load(in_xtc, top=in_pdb, stride=stride)\n",
    "\n",
    "    sel = \"not (\" + \" or \".join([f\"resname {r}\" for r in remove_resnames]) + \")\"\n",
    "    keep_idx = traj.topology.select(sel)\n",
    "\n",
    "    if keep_idx.size == 0:\n",
    "        raise RuntimeError(\"Selection resulted in 0 atoms. Check resnames / topology.\")\n",
    "\n",
    "    traj2 = traj.atom_slice(keep_idx)\n",
    "    traj2[0].save_pdb(out_pdb)\n",
    "    traj2.save_xtc(out_xtc)\n",
    "\n",
    "    return out_pdb, out_xtc\n",
    "\n",
    "\n",
    "\n",
    "# =====================\n",
    "in_pdb = '/mnt/hdd/jeff/dataset/output/mdgen-collagen/wh-wt-nc1atm/raw.pdb'\n",
    "in_xtc =  '/mnt/hdd/jeff/dataset/output/mdgen-collagen/wh-wt-nc1atm/raw.xtc'\n",
    "out_pdb = '/mnt/hdd/jeff/dataset/output/mdgen-collagen/wh-wt-nc1atm/no-water.pdb'\n",
    "out_xtc = '/mnt/hdd/jeff/dataset/output/mdgen-collagen/wh-wt-nc1atm/no-water.xtc'\n",
    "remove_water_from_pdb_xtc(\n",
    "    in_pdb,\n",
    "    in_xtc,\n",
    "    out_pdb,\n",
    "    out_xtc,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ba3926",
   "metadata": {},
   "source": [
    "#### residuetype - atom14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aed7e0e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HYP']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def in_pdb_residuetype_minus_atom14_residuetype(\n",
    "    pdb_path,\n",
    "    include_hetatm=True,\n",
    "    ignore_resnames=(\"HOH\", \"WAT\", \"SOL\", \"TIP3\", \"TIP3P\", \"SPC\", \"SPCE\"),\n",
    "):\n",
    "    import os\n",
    "    if not os.path.isfile(pdb_path):\n",
    "        raise FileNotFoundError(f\"PDB not found: {pdb_path}\")\n",
    "\n",
    "    atom14_set = {\n",
    "        \"ALA\",\"ARG\",\"ASN\",\"ASP\",\"CYS\",\"GLN\",\"GLU\",\"GLY\",\"HIS\",\"ILE\",\n",
    "        \"LEU\",\"LYS\",\"MET\",\"PHE\",\"PRO\",\"SER\",\"THR\",\"TRP\",\"TYR\",\"VAL\",\n",
    "    }\n",
    "    ignore_set = set(ignore_resnames)\n",
    "\n",
    "    in_pdb_set = set()\n",
    "    with open(pdb_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            is_atom = line.startswith(\"ATOM  \")\n",
    "            is_het = line.startswith(\"HETATM\")\n",
    "            if not (is_atom or (include_hetatm and is_het)):\n",
    "                continue\n",
    "            if len(line) < 20:\n",
    "                continue\n",
    "\n",
    "            resname = line[17:20].strip()\n",
    "            if not resname or resname in ignore_set:\n",
    "                continue\n",
    "\n",
    "            in_pdb_set.add(resname)\n",
    "\n",
    "    return sorted(in_pdb_set - atom14_set)\n",
    "\n",
    "\n",
    "# =====================\n",
    "pdb_path = '/mnt/hdd/jeff/dataset/output/mdgen-collagen/wh-wt-nc1atm/no-water.pdb'\n",
    "in_pdb_residuetype_minus_atom14_residuetype(\n",
    "    pdb_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b012e9bb",
   "metadata": {},
   "source": [
    "#### HYP->PRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2acdcbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/mnt/hdd/jeff/dataset/output/mdgen-collagen/wh-wt-nc1atm/HYPPRO.pdb',\n",
       " '/mnt/hdd/jeff/dataset/output/mdgen-collagen/wh-wt-nc1atm/HYPPRO.xtc',\n",
       " 4935)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rename_hyp_to_pro_in_pdb_xtc(\n",
    "    in_pdb,\n",
    "    in_xtc,\n",
    "    out_pdb,\n",
    "    out_xtc,\n",
    "):\n",
    "    \"\"\"\n",
    "    Input:  pdb + xtc\n",
    "    Output: pdb + xtc\n",
    "\n",
    "    做的事：\n",
    "    - 只把 PDB 裡面 residue name = \"HYP\" 的地方改成 \"PRO\"\n",
    "    - 其他內容完全不動\n",
    "    - XTC 本身不含胺基酸名字/拓撲資訊，所以直接原封不動複製一份到 out_xtc\n",
    "      （之後用 out_pdb 當拓撲配合 out_xtc 讀取即可）\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import shutil\n",
    "\n",
    "    if not os.path.isfile(in_pdb):\n",
    "        raise FileNotFoundError(f\"PDB not found: {in_pdb}\")\n",
    "    if not os.path.isfile(in_xtc):\n",
    "        raise FileNotFoundError(f\"XTC not found: {in_xtc}\")\n",
    "\n",
    "    n_changed = 0\n",
    "    with open(in_pdb, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fin, open(out_pdb, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for line in fin:\n",
    "            if line.startswith(\"ATOM  \") or line.startswith(\"HETATM\"):\n",
    "                if len(line) >= 20:\n",
    "                    resname = line[17:20]\n",
    "                    if resname.strip() == \"HYP\":\n",
    "                        line = line[:17] + \"PRO\" + line[20:]\n",
    "                        n_changed += 1\n",
    "            fout.write(line)\n",
    "\n",
    "    shutil.copyfile(in_xtc, out_xtc)\n",
    "\n",
    "    return out_pdb, out_xtc, n_changed\n",
    "\n",
    "# =====================\n",
    "in_pdb = '/mnt/hdd/jeff/dataset/output/mdgen-collagen/wh-wt-nc1atm/no-water.pdb'\n",
    "in_xtc = '/mnt/hdd/jeff/dataset/output/mdgen-collagen/wh-wt-nc1atm/no-water.xtc'\n",
    "out_pdb = '/mnt/hdd/jeff/dataset/output/mdgen-collagen/wh-wt-nc1atm/HYPPRO.pdb'\n",
    "out_xtc = '/mnt/hdd/jeff/dataset/output/mdgen-collagen/wh-wt-nc1atm/HYPPRO.xtc'\n",
    "rename_hyp_to_pro_in_pdb_xtc(\n",
    "    in_pdb,\n",
    "    in_xtc,\n",
    "    out_pdb,\n",
    "    out_xtc,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f251767c",
   "metadata": {},
   "source": [
    "#### pro reorder atom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3fcaf643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Folders: 100%|██████████| 80/80 [00:11<00:00,  6.86it/s]\n"
     ]
    }
   ],
   "source": [
    "import mdtraj as md\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "PRO_TARGET_ATOMS = ['N', 'CA', 'C', 'CB', 'O', 'CG', 'CD']\n",
    "\n",
    "def rebuild_traj_with_pro_order(traj):\n",
    "    old_top = traj.topology\n",
    "    new_top = md.Topology()\n",
    "\n",
    "    # 建立 mapping: old_atom_index -> new_atom_index\n",
    "    old_to_new = {}\n",
    "\n",
    "    for old_chain in old_top.chains:\n",
    "        new_chain = new_top.add_chain()\n",
    "\n",
    "        for old_res in old_chain.residues:\n",
    "            new_res = new_top.add_residue(old_res.name, new_chain, resSeq=old_res.resSeq)\n",
    "\n",
    "            if old_res.name == 'PRO':\n",
    "                atom_dict = {a.name: a for a in old_res.atoms}\n",
    "                missing = [name for name in PRO_TARGET_ATOMS if name not in atom_dict]\n",
    "                if missing:\n",
    "                    return None, f\"PRO residue {old_res.index} missing atoms: {missing}\"\n",
    "\n",
    "                # 只加入 7 顆，且固定順序\n",
    "                for name in PRO_TARGET_ATOMS:\n",
    "                    old_atom = atom_dict[name]\n",
    "                    new_atom = new_top.add_atom(old_atom.name, old_atom.element, new_res)\n",
    "                    old_to_new[old_atom.index] = new_atom.index\n",
    "            else:\n",
    "                # 其他殘基：完全照原本順序加入（包含 H）\n",
    "                for old_atom in old_res.atoms:\n",
    "                    new_atom = new_top.add_atom(old_atom.name, old_atom.element, new_res)\n",
    "                    old_to_new[old_atom.index] = new_atom.index\n",
    "\n",
    "    # 複製 bonds（可有可無，但保留比較完整）\n",
    "    for bond in old_top.bonds:\n",
    "        a1, a2 = bond\n",
    "        if a1.index in old_to_new and a2.index in old_to_new:\n",
    "            new_top.add_bond(\n",
    "                new_top.atom(old_to_new[a1.index]),\n",
    "                new_top.atom(old_to_new[a2.index])\n",
    "            )\n",
    "\n",
    "    # 依照 new_top 的 atom 順序，建立 old indices list（用來重排 xyz）\n",
    "    # old_to_new 是 old->new，所以我們反過來做 new->old\n",
    "    new_to_old = [None] * new_top.n_atoms\n",
    "    for old_i, new_i in old_to_new.items():\n",
    "        new_to_old[new_i] = old_i\n",
    "\n",
    "    if any(v is None for v in new_to_old):\n",
    "        return None, \"Atom mapping incomplete (unexpected).\"\n",
    "\n",
    "    new_xyz = traj.xyz[:, new_to_old, :]\n",
    "\n",
    "    new_traj = md.Trajectory(\n",
    "        xyz=new_xyz,\n",
    "        topology=new_top,\n",
    "        time=traj.time,\n",
    "        unitcell_lengths=traj.unitcell_lengths,\n",
    "        unitcell_angles=traj.unitcell_angles\n",
    "    )\n",
    "    return new_traj, None\n",
    "\n",
    "\n",
    "def process_and_copy_folders(input_dir, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    subfolders = [f.name for f in os.scandir(input_dir) if f.is_dir()]\n",
    "\n",
    "    for folder_name in tqdm(subfolders, desc=\"Processing Folders\"):\n",
    "        in_subfolder = os.path.join(input_dir, folder_name)\n",
    "        out_subfolder = os.path.join(output_dir, folder_name)\n",
    "\n",
    "        pdb_file = next((f for f in os.listdir(in_subfolder) if f.endswith(\".pdb\")), None)\n",
    "        xtc_file = next((f for f in os.listdir(in_subfolder) if f.endswith(\".xtc\")), None)\n",
    "        if not pdb_file or not xtc_file:\n",
    "            continue\n",
    "\n",
    "        if not os.path.exists(out_subfolder):\n",
    "            os.makedirs(out_subfolder)\n",
    "\n",
    "        try:\n",
    "            in_pdb_path = os.path.join(in_subfolder, pdb_file)\n",
    "            in_xtc_path = os.path.join(in_subfolder, xtc_file)\n",
    "            out_pdb_path = os.path.join(out_subfolder, pdb_file)\n",
    "            out_xtc_path = os.path.join(out_subfolder, xtc_file)\n",
    "\n",
    "            traj = md.load(in_xtc_path, top=in_pdb_path)\n",
    "\n",
    "            new_traj, err = rebuild_traj_with_pro_order(traj)\n",
    "            if err is not None:\n",
    "                print(f\"\\n[Skip] {folder_name}: {err}\")\n",
    "                continue\n",
    "\n",
    "            new_traj[0].save_pdb(out_pdb_path)\n",
    "            new_traj.save_xtc(out_xtc_path)\n",
    "\n",
    "            for file in os.listdir(in_subfolder):\n",
    "                if not file.endswith(\".pdb\") and not file.endswith(\".xtc\"):\n",
    "                    shutil.copy2(os.path.join(in_subfolder, file),\n",
    "                                 os.path.join(out_subfolder, file))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing {folder_name}: {e}\")\n",
    "\n",
    "\n",
    "input_folder = '/mnt/hdd/jeff/dataset/output/mdgen-collagen/segment'\n",
    "output_folder = '/mnt/hdd/jeff/dataset/output/mdgen-collagen/segmentPRO'\n",
    "process_and_copy_folders(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4095540f",
   "metadata": {},
   "source": [
    "#### residue id start from 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d84c2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Renumber residues: 100%|██████████| 80/80 [00:10<00:00,  7.34it/s]\n"
     ]
    }
   ],
   "source": [
    "import mdtraj as md\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "def renumber_residues_from_1(input_dir, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    subfolders = [f.name for f in os.scandir(input_dir) if f.is_dir()]\n",
    "\n",
    "    for folder_name in tqdm(subfolders, desc=\"Renumber residues\"):\n",
    "        in_subfolder = os.path.join(input_dir, folder_name)\n",
    "        out_subfolder = os.path.join(output_dir, folder_name)\n",
    "        if not os.path.exists(out_subfolder):\n",
    "            os.makedirs(out_subfolder)\n",
    "\n",
    "        pdb_file = next((f for f in os.listdir(in_subfolder) if f.endswith(\".pdb\")), None)\n",
    "        xtc_file = next((f for f in os.listdir(in_subfolder) if f.endswith(\".xtc\")), None)\n",
    "        if not pdb_file or not xtc_file:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            in_pdb_path = os.path.join(in_subfolder, pdb_file)\n",
    "            in_xtc_path = os.path.join(in_subfolder, xtc_file)\n",
    "            out_pdb_path = os.path.join(out_subfolder, pdb_file)\n",
    "            out_xtc_path = os.path.join(out_subfolder, xtc_file)\n",
    "\n",
    "            traj = md.load(in_xtc_path, top=in_pdb_path)\n",
    "            old_top = traj.topology\n",
    "\n",
    "            # --- rebuild topology but renumber residues from 1 (per chain) ---\n",
    "            new_top = md.Topology()\n",
    "            old_to_new_atom = {}\n",
    "\n",
    "            for old_chain in old_top.chains:\n",
    "                new_chain = new_top.add_chain()\n",
    "                new_resSeq = 1\n",
    "\n",
    "                for old_res in old_chain.residues:\n",
    "                    new_res = new_top.add_residue(\n",
    "                        old_res.name,\n",
    "                        new_chain,\n",
    "                        resSeq=new_resSeq\n",
    "                    )\n",
    "                    new_resSeq += 1\n",
    "\n",
    "                    for old_atom in old_res.atoms:\n",
    "                        new_atom = new_top.add_atom(old_atom.name, old_atom.element, new_res)\n",
    "                        old_to_new_atom[old_atom.index] = new_atom.index\n",
    "\n",
    "            # copy bonds\n",
    "            for bond in old_top.bonds:\n",
    "                a1, a2 = bond\n",
    "                if a1.index in old_to_new_atom and a2.index in old_to_new_atom:\n",
    "                    new_top.add_bond(\n",
    "                        new_top.atom(old_to_new_atom[a1.index]),\n",
    "                        new_top.atom(old_to_new_atom[a2.index])\n",
    "                    )\n",
    "\n",
    "            new_traj = md.Trajectory(\n",
    "                xyz=traj.xyz,\n",
    "                topology=new_top,\n",
    "                time=traj.time,\n",
    "                unitcell_lengths=traj.unitcell_lengths,\n",
    "                unitcell_angles=traj.unitcell_angles\n",
    "            )\n",
    "\n",
    "            # save\n",
    "            new_traj[0].save_pdb(out_pdb_path)\n",
    "            new_traj.save_xtc(out_xtc_path)\n",
    "\n",
    "            # copy other files\n",
    "            for file in os.listdir(in_subfolder):\n",
    "                if not file.endswith(\".pdb\") and not file.endswith(\".xtc\"):\n",
    "                    shutil.copy2(os.path.join(in_subfolder, file),\n",
    "                                 os.path.join(out_subfolder, file))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing {folder_name}: {e}\")\n",
    "\n",
    "# example\n",
    "input_folder = '/mnt/hdd/jeff/dataset/output/mdgen-collagen/segmentPRO'\n",
    "output_folder = '/mnt/hdd/jeff/dataset/output/mdgen-collagen/segment_order'\n",
    "renumber_residues_from_1(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cace9c93",
   "metadata": {},
   "source": [
    "#### check xtc frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03caa786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_frames_in_pdb_xtc(in_pdb, in_xtc):\n",
    "    import os\n",
    "    if not os.path.isfile(in_pdb):\n",
    "        raise FileNotFoundError(f\"PDB not found: {in_pdb}\")\n",
    "    if not os.path.isfile(in_xtc):\n",
    "        raise FileNotFoundError(f\"XTC not found: {in_xtc}\")\n",
    "\n",
    "    import mdtraj as md\n",
    "\n",
    "    traj = md.load(in_xtc, top=in_pdb)\n",
    "    return traj.n_frames\n",
    "\n",
    "# ====================\n",
    "in_pdb = '/mnt/hdd/jeff/dataset/output/mdgen-collagen/segment/1-256/1-256.pdb'\n",
    "in_xtc = '/mnt/hdd/jeff/dataset/output/mdgen-collagen/segment/1-256/1-256.xtc'\n",
    "count_frames_in_pdb_xtc(in_pdb,in_xtc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43ca71b",
   "metadata": {},
   "source": [
    "### csv split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0be5e03",
   "metadata": {},
   "source": [
    "#### making csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fd87878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/hdd/jeff/dataset/output/mdgen-collagen/wh-wt-nc1atm/HYPPRO_split.csv'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_pdb_to_seq_csv(\n",
    "    in_pdb,\n",
    "    out_csv,\n",
    "    seg_len=256,\n",
    "    n_segments=80,\n",
    "    overlap=True,\n",
    "    include_hetatm=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Input:  pdb\n",
    "    Output: csv with columns: name, seqres\n",
    "\n",
    "    - 以「殘基(residue)」為單位切序列\n",
    "    - name 格式：\"{start}-{end}\"（1-based, inclusive）\n",
    "      例如 1-256\n",
    "    - seqres：該段的一字母胺基酸序列\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import csv\n",
    "\n",
    "    if not os.path.isfile(in_pdb):\n",
    "        raise FileNotFoundError(f\"PDB not found: {in_pdb}\")\n",
    "\n",
    "    # 3-letter -> 1-letter\n",
    "    restype_3to1 = {\n",
    "        \"ALA\": \"A\", \"ARG\": \"R\", \"ASN\": \"N\", \"ASP\": \"D\", \"CYS\": \"C\",\n",
    "        \"GLN\": \"Q\", \"GLU\": \"E\", \"GLY\": \"G\", \"HIS\": \"H\", \"ILE\": \"I\",\n",
    "        \"LEU\": \"L\", \"LYS\": \"K\", \"MET\": \"M\", \"PHE\": \"F\", \"PRO\": \"P\",\n",
    "        \"SER\": \"S\", \"THR\": \"T\", \"TRP\": \"W\", \"TYR\": \"Y\", \"VAL\": \"V\",\n",
    "    }\n",
    "\n",
    "    # 解析 PDB：依照 (chain_id, resSeq, iCode, resname) 收集殘基順序\n",
    "    residues = []\n",
    "    seen = set()\n",
    "\n",
    "    with open(in_pdb, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"ATOM  \") or (include_hetatm and line.startswith(\"HETATM\")):\n",
    "                if len(line) < 54:\n",
    "                    continue\n",
    "                resname = line[17:20].strip()\n",
    "                chain_id = line[21].strip() or \"_\"\n",
    "                resseq = line[22:26].strip()\n",
    "                icode = line[26].strip() or \"_\"\n",
    "                if not resseq:\n",
    "                    continue\n",
    "                key = (chain_id, resseq, icode)\n",
    "                if key in seen:\n",
    "                    continue\n",
    "                seen.add(key)\n",
    "                residues.append(resname)\n",
    "\n",
    "    n_res = len(residues)\n",
    "    if n_res == 0:\n",
    "        raise RuntimeError(\"No residues parsed from PDB. Check file format.\")\n",
    "    if n_res < seg_len:\n",
    "        raise ValueError(f\"Sequence too short ({n_res}) for seg_len={seg_len}\")\n",
    "\n",
    "    if n_segments < 1:\n",
    "        raise ValueError(\"n_segments must be >= 1\")\n",
    "\n",
    "    if overlap:\n",
    "        if n_segments == 1:\n",
    "            step = 0\n",
    "        else:\n",
    "            step = (n_res - seg_len) // (n_segments - 1)\n",
    "            if step < 1:\n",
    "                step = 1\n",
    "    else:\n",
    "        step = seg_len\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_csv) or \".\", exist_ok=True)\n",
    "\n",
    "    with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"name\", \"seqres\"])\n",
    "\n",
    "        for i in range(n_segments):\n",
    "            start0 = i * step\n",
    "            end0 = start0 + seg_len\n",
    "            if end0 > n_res:\n",
    "                end0 = n_res\n",
    "                start0 = end0 - seg_len\n",
    "\n",
    "            # 1-based inclusive range for name\n",
    "            start1 = start0 + 1\n",
    "            end1 = end0\n",
    "            name = f\"{start1}-{end1}\"\n",
    "\n",
    "            seq = \"\".join(restype_3to1.get(rn, \"X\") for rn in residues[start0:end0])\n",
    "            writer.writerow([name, seq])\n",
    "\n",
    "    return out_csv\n",
    "\n",
    "# =====================\n",
    "in_pdb = '/mnt/hdd/jeff/dataset/output/mdgen-collagen/wh-wt-nc1atm/HYPPRO.pdb'\n",
    "out_csv = '/mnt/hdd/jeff/dataset/output/mdgen-collagen/wh-wt-nc1atm/HYPPRO_split.csv'\n",
    "split_pdb_to_seq_csv(\n",
    "    in_pdb,\n",
    "    out_csv,\n",
    "    seg_len=256,\n",
    "    n_segments=80,\n",
    "    overlap=True,\n",
    "    include_hetatm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83afa6d",
   "metadata": {},
   "source": [
    "#### making segment pdb xtc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65484920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_pdb_xtc_by_csv_ranges(\n",
    "    in_pdb,\n",
    "    in_xtc,\n",
    "    in_csv,\n",
    "    save_folder,\n",
    "    stride=1,\n",
    "    protein_only=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    依照 csv 的 name 欄位（格式：start-end，例如 1-256）去切 pdb/xtc，\n",
    "    並輸出到：\n",
    "      save_folder/start-end/start-end.pdb\n",
    "      save_folder/start-end/start-end.xtc\n",
    "\n",
    "    假設：\n",
    "    - csv 至少有欄位：name, seqres（seqres 這裡不會用到，只用 name）\n",
    "    - start/end 是 1-based，且 end 是 inclusive\n",
    "    - protein_only=True 時，start/end 是以「protein residues 的順序」計數\n",
    "      (也就是 mdtraj topology 裡 r.is_protein 的那串殘基)\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import csv\n",
    "    import mdtraj as md\n",
    "\n",
    "    if not os.path.isfile(in_pdb):\n",
    "        raise FileNotFoundError(f\"PDB not found: {in_pdb}\")\n",
    "    if not os.path.isfile(in_xtc):\n",
    "        raise FileNotFoundError(f\"XTC not found: {in_xtc}\")\n",
    "    if not os.path.isfile(in_csv):\n",
    "        raise FileNotFoundError(f\"CSV not found: {in_csv}\")\n",
    "\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "    traj = md.load(in_xtc, top=in_pdb, stride=stride)\n",
    "    topo = traj.topology\n",
    "\n",
    "    if protein_only:\n",
    "        res_list = [r.index for r in topo.residues if r.is_protein]\n",
    "    else:\n",
    "        res_list = [r.index for r in topo.residues]\n",
    "\n",
    "    n_res = len(res_list)\n",
    "    if n_res == 0:\n",
    "        raise RuntimeError(\"No residues found (protein_only setting may be wrong).\")\n",
    "\n",
    "    written = []\n",
    "\n",
    "    with open(in_csv, \"r\", encoding=\"utf-8\", errors=\"ignore\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        if \"name\" not in reader.fieldnames:\n",
    "            raise ValueError(f\"CSV must contain a 'name' column, got: {reader.fieldnames}\")\n",
    "\n",
    "        for row in reader:\n",
    "            name = (row.get(\"name\") or \"\").strip()\n",
    "            if not name:\n",
    "                continue\n",
    "\n",
    "            # parse \"start-end\"\n",
    "            if \"-\" not in name:\n",
    "                raise ValueError(f\"Bad name format (expected start-end): {name}\")\n",
    "            s, e = name.split(\"-\", 1)\n",
    "            start = int(s)\n",
    "            end = int(e)\n",
    "\n",
    "            if start < 1 or end < start:\n",
    "                raise ValueError(f\"Invalid range: {name}\")\n",
    "            if end > n_res:\n",
    "                raise ValueError(f\"Range {name} exceeds available residues (n_res={n_res})\")\n",
    "\n",
    "            # convert to 0-based slice over res_list\n",
    "            start0 = start - 1\n",
    "            end0 = end  # python slice end is exclusive, but end is inclusive -> keep end0=end\n",
    "            selected_res_indices = set(res_list[start0:end0])\n",
    "\n",
    "            atom_indices = [a.index for a in topo.atoms if a.residue.index in selected_res_indices]\n",
    "            if len(atom_indices) == 0:\n",
    "                raise RuntimeError(f\"Selection produced 0 atoms for range {name}\")\n",
    "\n",
    "            seg_traj = traj.atom_slice(atom_indices)\n",
    "\n",
    "            seg_dir = os.path.join(save_folder, name)\n",
    "            os.makedirs(seg_dir, exist_ok=True)\n",
    "\n",
    "            out_pdb = os.path.join(seg_dir, f\"{name}.pdb\")\n",
    "            out_xtc = os.path.join(seg_dir, f\"{name}.xtc\")\n",
    "\n",
    "            seg_traj[0].save_pdb(out_pdb)\n",
    "            seg_traj.save_xtc(out_xtc)\n",
    "\n",
    "            written.append((name, out_pdb, out_xtc))\n",
    "\n",
    "    return written\n",
    "\n",
    "\n",
    "# =====================\n",
    "in_pdb = '/mnt/hdd/jeff/dataset/output/mdgen-collagen/wh-wt-nc1atm/HYPPRO.pdb'\n",
    "in_xtc = '/mnt/hdd/jeff/dataset/output/mdgen-collagen/wh-wt-nc1atm/HYPPRO.xtc'\n",
    "in_csv = '/mnt/hdd/jeff/dataset/output/mdgen-collagen/wh-wt-nc1atm/HYPPRO_split.csv'\n",
    "save_folder = '/mnt/hdd/jeff/dataset/output/mdgen-collagen/segment'\n",
    "cut_pdb_xtc_by_csv_ranges(\n",
    "    in_pdb,\n",
    "    in_xtc,\n",
    "    in_csv,\n",
    "    save_folder,\n",
    "    stride=1,\n",
    "    protein_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6978c85",
   "metadata": {},
   "source": [
    "### preprocess-npy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b476a4eb",
   "metadata": {},
   "source": [
    "#### npy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f373b8",
   "metadata": {},
   "source": [
    "python -m scripts.prep_sims#4 --split /mnt/hdd/jeff/dataset/output/mdgen-collagen/wh-wt-nc1atm/HYPPRO_split.csv --sim_dir /mnt/hdd/jeff/dataset/output/mdgen-collagen/segment --outdir /mnt/hdd/jeff/dataset/output/mdgen-collagen/npy --num_workers 4  --stride 40 --atlas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec00b13d",
   "metadata": {},
   "source": [
    "#### check npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428d36fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_npy_info(npy_path, max_items=20, max_width=120):\n",
    "    import os\n",
    "    if not os.path.isfile(npy_path):\n",
    "        raise FileNotFoundError(f\"NPY not found: {npy_path}\")\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    arr = np.load(npy_path, allow_pickle=True)\n",
    "    print(\"path:\", npy_path)\n",
    "    print(\"dtype:\", arr.dtype)\n",
    "    print(\"shape:\", arr.shape)\n",
    "\n",
    "    # 內容預覽\n",
    "    np.set_printoptions(threshold=max_items, linewidth=max_width)\n",
    "    print(\"content preview:\")\n",
    "    print(arr)\n",
    "    return arr\n",
    "\n",
    "# ==================\n",
    "npy_path = '/mnt/hdd/jeff/dataset/output/mdgen-collagen/npy/1-256.npy'\n",
    "print_npy_info(npy_path, max_items=20, max_width=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceb6535",
   "metadata": {},
   "source": [
    "### inference model : mdgen-ckpt-from-paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1738ed70",
   "metadata": {},
   "source": [
    "python sim_inference#4.py --sim_ckpt /mnt/hdd/jeff/dataset/output/mdgen-#1/ckpt/epoch=999-step=111000.ckpt --data_dir /mnt/hdd/jeff/dataset/output/mdgen-collagen/npy --num_frames 250 --num_rollouts 1 --split /mnt/hdd/jeff/dataset/output/mdgen-collagen/wh-wt-nc1atm/HYPPRO_split.csv --out_dir /mnt/hdd/jeff/dataset/output/mdgen-collagen/inference --xtc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8666034b",
   "metadata": {},
   "source": [
    "### metrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a022d3",
   "metadata": {},
   "source": [
    "#### residue number+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0464f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_residue_number_plus_one_in_folder(folder_path, recursive=True):\n",
    "    \"\"\"\n",
    "    直接在資料夾內「就地修改」所有 .pdb：把 PDB 的 residue number(resSeq, 欄位 23–26) 全部 +1\n",
    "    - .xtc 本身沒有 residue number（只存座標），所以不改；只會順便檢查同名 .xtc 是否存在。\n",
    "    \"\"\"\n",
    "    import os\n",
    "\n",
    "    if not os.path.isdir(folder_path):\n",
    "        raise NotADirectoryError(f\"Folder not found: {folder_path}\")\n",
    "\n",
    "    def iter_pdb_files(root):\n",
    "        if recursive:\n",
    "            for dp, _, fns in os.walk(root):\n",
    "                for fn in fns:\n",
    "                    if fn.lower().endswith(\".pdb\"):\n",
    "                        yield os.path.join(dp, fn)\n",
    "        else:\n",
    "            for fn in os.listdir(root):\n",
    "                if fn.lower().endswith(\".pdb\"):\n",
    "                    yield os.path.join(root, fn)\n",
    "\n",
    "    changed = []\n",
    "    for pdb_path in iter_pdb_files(folder_path):\n",
    "        with open(pdb_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        n_changed = 0\n",
    "        out_lines = []\n",
    "\n",
    "        for line in lines:\n",
    "            if (line.startswith(\"ATOM  \") or line.startswith(\"HETATM\")) and len(line) >= 27:\n",
    "                # PDB fixed columns: resSeq is columns 23-26 -> 0-based [22:26]\n",
    "                resseq_raw = line[22:26]\n",
    "                resseq_str = resseq_raw.strip()\n",
    "                if resseq_str:\n",
    "                    try:\n",
    "                        resseq = int(resseq_str) + 1\n",
    "                        line = line[:22] + f\"{resseq:4d}\" + line[26:]\n",
    "                        n_changed += 1\n",
    "                    except:\n",
    "                        pass\n",
    "            out_lines.append(line)\n",
    "\n",
    "        if n_changed > 0:\n",
    "            with open(pdb_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.writelines(out_lines)\n",
    "\n",
    "        base = os.path.splitext(pdb_path)[0]\n",
    "        xtc_path = base + \".xtc\"\n",
    "        has_xtc = os.path.isfile(xtc_path)\n",
    "\n",
    "        changed.append({\n",
    "            \"pdb\": pdb_path,\n",
    "            \"xtc\": xtc_path,\n",
    "            \"xtc_exists\": has_xtc,\n",
    "            \"pdb_lines_modified\": n_changed,\n",
    "        })\n",
    "\n",
    "    return changed\n",
    "\n",
    "\n",
    "folder_path = '/mnt/hdd/jeff/dataset/output/mdgen-collagen/inference'\n",
    "shift_residue_number_plus_one_in_folder(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f3d203",
   "metadata": {},
   "source": [
    "#### metrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8186c237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "inference_dir = '/mnt/hdd/jeff/dataset/output/mdgen-collagen/inference'\n",
    "csv = '/mnt/hdd/jeff/dataset/output/mdgen-collagen/wh-wt-nc1atm/HYPPRO_split.csv'\n",
    "log_dir = '/mnt/hdd/jeff/mdgen-#1/bin/log/collagen-0113'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "## making list\n",
    "df = pd.read_csv(csv)\n",
    "names = df['name'].tolist()\n",
    "#壞掉的\n",
    "broken_pdb = [\"901-1156\",\"937-1192\"]\n",
    "names = [n for n in names if str(n) not in broken_pdb] \n",
    "#已經做好的\n",
    "pkl_dir = \"/mnt/hdd/jeff/dataset/output/mdgen-collagen/pkl\" \n",
    "done = {os.path.splitext(fn)[0] for fn in os.listdir(pkl_dir)}\n",
    "names = [n for n in names if str(n) not in done]\n",
    "\n",
    "for name in tqdm(names, desc=\"Running Analysis\"):\n",
    "    print(name)\n",
    "    command = [\n",
    "        \"python\", \"-m\", \"scripts.analyze_ensembles#4\",\n",
    "        \"--atlas_dir\", \"/mnt/hdd/jeff/dataset/output/mdgen-collagen/segment_order\",\n",
    "        \"--pkl\", \"/mnt/hdd/jeff/dataset/output/mdgen-collagen/pkl\",\n",
    "        \"--pdbdir\", inference_dir,\n",
    "        \"--pdb_id\", name,\n",
    "        \"--num_workers\", \"4\"\n",
    "    ]\n",
    "\n",
    "    log_path = os.path.join(log_dir, f\"{name}.log\")\n",
    "    try:\n",
    "        with open(log_path, \"w\") as f:\n",
    "            subprocess.run(command, check=True, stdout=f, stderr=subprocess.STDOUT, text=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error analyzing {name}. See log: {log_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eb2e71",
   "metadata": {},
   "source": [
    "#### 移除沒算成功的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d46c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os \n",
    "in_dir = \"/mnt/hdd/jeff/dataset/output/mdgen-collagen/pkl\"\n",
    "out_dir = \"/mnt/hdd/jeff/dataset/output/mdgen-collagen/clean-pkl\"\n",
    "\n",
    "for name in os.listdir(in_dir):\n",
    "    inp = os.path.join(in_dir,name)\n",
    "    outp = os.path.join(out_dir,name)\n",
    "\n",
    "    with open(inp, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    clean = {k: v for k, v in data.items() if isinstance(v, dict) and (\"error\" not in v)}\n",
    "\n",
    "    print(\"before:\", len(data), \"after:\", len(clean))\n",
    "\n",
    "    with open(outp, \"wb\") as f:\n",
    "        pickle.dump(clean, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ece4b10",
   "metadata": {},
   "source": [
    "#### 多個pkl合併為一個"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e915a1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 78 files -> /mnt/hdd/jeff/dataset/output/mdgen-collagen/print-pkl/merged.pkl\n",
      "Total targets in merged dict: 78\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "def merge_pkl_folder(pkl_folder, out_pkl_path):\n",
    "    \"\"\"\n",
    "    把資料夾內所有 .pkl 合併成一個 .pkl\n",
    "    輸出格式會是：{name: out_dict, name2: out_dict2, ...}\n",
    "    以符合 scripts.print_analysis 的 analyze_data(data) 期待的結構。\n",
    "    \"\"\"\n",
    "    pkl_files = sorted(glob.glob(os.path.join(pkl_folder, \"*.pkl\")))\n",
    "    if not pkl_files:\n",
    "        raise FileNotFoundError(f\"No .pkl files found in: {pkl_folder}\")\n",
    "\n",
    "    merged = {}\n",
    "    for fp in pkl_files:\n",
    "        with open(fp, \"rb\") as f:\n",
    "            obj = pickle.load(f)\n",
    "\n",
    "        # obj 正常情況：dict(name -> out_dict)\n",
    "        if isinstance(obj, dict):\n",
    "            # 直接把這包的所有 (name -> out_dict) 展開進 merged\n",
    "            # （避免你之前多包一層造成 KeyError）\n",
    "            for k, v in obj.items():\n",
    "                merged[k] = v\n",
    "        else:\n",
    "            # 很少見：如果某些檔案不是 dict，就略過或報錯\n",
    "            raise ValueError(f\"{fp} is not a dict. Got type={type(obj)}\")\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_pkl_path), exist_ok=True)\n",
    "    with open(out_pkl_path, \"wb\") as f:\n",
    "        pickle.dump(merged, f)\n",
    "\n",
    "    print(f\"Merged {len(pkl_files)} files -> {out_pkl_path}\")\n",
    "    print(f\"Total targets in merged dict: {len(merged)}\")\n",
    "\n",
    "# =====================\n",
    "pkl_folder = '/mnt/hdd/jeff/dataset/output/mdgen-collagen/pkl'\n",
    "out_pkl = '/mnt/hdd/jeff/dataset/output/mdgen-collagen/print-pkl/merged.pkl'\n",
    "merge_pkl_folder(pkl_folder, out_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cf9760",
   "metadata": {},
   "source": [
    "python -m scripts.print_analysis /mnt/hdd/jeff/dataset/output/mdgen-collagen/print-pkl/merged.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc17bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_abs_time_emb_from_ckpt(ckpt_path, trust_ckpt=True):\n",
    "    import os\n",
    "    import torch\n",
    "    import argparse\n",
    "\n",
    "    if not os.path.isfile(ckpt_path):\n",
    "        raise FileNotFoundError(f\"ckpt not found: {ckpt_path}\")\n",
    "\n",
    "    def _extract_state_dict(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            if \"state_dict\" in obj and isinstance(obj[\"state_dict\"], dict):\n",
    "                return obj[\"state_dict\"]\n",
    "            # some checkpoints may directly be a state_dict-like dict\n",
    "            return obj\n",
    "        return {}\n",
    "\n",
    "    # --- 1) Try safest path: weights_only=True (no arbitrary code execution) ---\n",
    "    ckpt = None\n",
    "    state_dict = {}\n",
    "    load_mode = None\n",
    "    try:\n",
    "        ckpt = torch.load(ckpt_path, map_location=\"cpu\", weights_only=True)\n",
    "        state_dict = _extract_state_dict(ckpt)\n",
    "        load_mode = \"weights_only=True\"\n",
    "    except Exception as e1:\n",
    "        # --- 2) If user trusts the file, allowlist argparse.Namespace and load fully ---\n",
    "        if not trust_ckpt:\n",
    "            raise RuntimeError(\n",
    "                \"Failed to load with weights_only=True. \"\n",
    "                \"Set trust_ckpt=True only if you trust the ckpt source.\\n\"\n",
    "                f\"Original error: {e1}\"\n",
    "            )\n",
    "        try:\n",
    "            # allowlist the class that blocks unpickling\n",
    "            torch.serialization.add_safe_globals([argparse.Namespace])\n",
    "            ckpt = torch.load(ckpt_path, map_location=\"cpu\", weights_only=False)\n",
    "            state_dict = _extract_state_dict(ckpt)\n",
    "            load_mode = \"weights_only=False (allowlisted argparse.Namespace)\"\n",
    "        except Exception as e2:\n",
    "            raise RuntimeError(\n",
    "                \"Failed to load checkpoint in both safe and trusted modes.\\n\"\n",
    "                f\"Safe-mode error: {e1}\\nTrusted-mode error: {e2}\"\n",
    "            )\n",
    "\n",
    "    # --- check time embedding existence ---\n",
    "    keys = list(state_dict.keys())\n",
    "    has_time_embed_key = any(k.endswith(\"time_embed\") or \".time_embed\" in k for k in keys)\n",
    "\n",
    "    # --- also try to read flags if present (best-effort) ---\n",
    "    abs_time_emb_val = None\n",
    "    no_rope_val = None\n",
    "    num_frames_val = None\n",
    "\n",
    "    if isinstance(ckpt, dict):\n",
    "        hp = ckpt.get(\"hyper_parameters\", {})\n",
    "        args = hp.get(\"args\", hp)\n",
    "\n",
    "        if isinstance(args, dict):\n",
    "            abs_time_emb_val = args.get(\"abs_time_emb\", None)\n",
    "            no_rope_val = args.get(\"no_rope\", None)\n",
    "            num_frames_val = args.get(\"num_frames\", None)\n",
    "        else:\n",
    "            # sometimes args is a Namespace or an object\n",
    "            abs_time_emb_val = getattr(args, \"abs_time_emb\", None)\n",
    "            no_rope_val = getattr(args, \"no_rope\", None)\n",
    "            num_frames_val = getattr(args, \"num_frames\", None)\n",
    "\n",
    "    inferred_abs_time_emb = bool(abs_time_emb_val) if abs_time_emb_val is not None else bool(has_time_embed_key)\n",
    "\n",
    "    result = {\n",
    "        \"ckpt_path\": ckpt_path,\n",
    "        \"load_mode\": load_mode,\n",
    "        \"has_time_embed_key_in_state_dict\": has_time_embed_key,\n",
    "        \"abs_time_emb_in_hparams\": abs_time_emb_val,   # may be None\n",
    "        \"inferred_abs_time_emb\": inferred_abs_time_emb,\n",
    "        \"no_rope_in_hparams\": no_rope_val,             # may be None\n",
    "        \"num_frames_in_hparams\": num_frames_val,       # may be None\n",
    "        \"n_state_dict_keys\": len(keys),\n",
    "        \"example_state_dict_keys\": keys[:30],\n",
    "    }\n",
    "\n",
    "    print(\"=== CKPT CHECK ===\")\n",
    "    print(\"ckpt:\", ckpt_path)\n",
    "    print(\"loaded via:\", load_mode)\n",
    "    print(\"time_embed key in state_dict:\", has_time_embed_key)\n",
    "    print(\"abs_time_emb in hparams:\", abs_time_emb_val)\n",
    "    print(\"=> inferred abs_time_emb:\", inferred_abs_time_emb)\n",
    "    print(\"no_rope in hparams:\", no_rope_val)\n",
    "    print(\"num_frames in hparams:\", num_frames_val)\n",
    "\n",
    "    return result\n",
    "check_abs_time_emb_from_ckpt('/mnt/hdd/jeff/dataset/output/mdgen-#0/ckpt/atlas.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
